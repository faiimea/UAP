# -*- coding: utf-8 -*-
"""UAP.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1Lg9oxnaS2zkD-jfRtBhcrIKN9qkZuD__
"""

import numpy as np
from torch.autograd import Variable
import torch as torch
import copy
from tqdm import tqdm
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from utils import get_subtraining_dataloader_cifar10,get_test_dataloader_cifar10
from model_structure import get_model
import random


def deepfool(image, net, num_classes = 10, overshoot = 0.02, max_iter = 50):
    """
    :param image: Image of size 3*H*W
    :param net: network (input: images, output: values of activation **BEFORE** softmax).
    :param num_class:
    :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).
    :param max_iter:
    :return:minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed image
    """
    # net.zero_grad()
    f_image = net(Variable(image, requires_grad = True)).data.cpu().numpy().flatten()
    I = f_image.argsort()[::-1] # 从小到大排序, 再从后往前复制一遍，So相当于从大到小排序
    I = I[0:num_classes] # 挑最大的num_classes个(从0开始，到num_classes结束)
    label = I[0] # 最大的判断的分类

    input_shape = image.detach().cpu().numpy().shape # 原始照片
    pert_image = copy.deepcopy(image) # 干扰照片
    w = np.zeros(input_shape)
    r_tot = np.zeros(input_shape)

    loop_i = 0

    x = Variable(pert_image, requires_grad = True)
    # net.zero_grad()
    fs = net(x)
    k_i = label
 

    while k_i == label and loop_i < max_iter: # 直到分错类别或者达到循环上限次数
        pert = np.inf
        fs[0,I[0]].backward(retain_graph=True) # x产生了grad
        grad_orig = x.grad.data.cpu().numpy().copy() # original grad
        for k in range(1, num_classes):
            if x.grad is not None:
                x.grad.zero_()

            fs[0,I[k]].backward(retain_graph=True)
            cur_grad = x.grad.data.cpu().numpy().copy() # current grad(分类为k(不是目前所划分的那一类)的grad

            # set new w_k and new f_k
            w_k = cur_grad - grad_orig
            f_k = (fs[0,I[k]] - fs[0,I[0]]).data.cpu().numpy()

            pert_k = abs(f_k) / np.linalg.norm(w_k.flatten())

            # determine which w_k to use
            if pert_k < pert: # 要找到最小的pert
                pert = pert_k
                w = w_k


        # compute r_i and r_tot
        r_i = (pert + 1e-4) * w / np.linalg.norm(w) # 这一次迭代的r
        r_tot = np.float32(r_tot + r_i) #r_total
        pert_image = image + (1+overshoot) * torch.from_numpy(r_tot).cuda()

        x = Variable(pert_image, requires_grad = True) # 将添加干扰后的图片输入net里面
        fs = net(x)
        k_i = np.argmax(fs.data.cpu().numpy().flatten()) # 选择最大的一个作为新的分类
        loop_i += 1
        if(loop_i == max_iter):
          print("can't find an adversarial example")
       

    r_tot = (1+overshoot)*r_tot
    return r_tot, loop_i

def proj_lp(v, xi, p):

    # Project on the lp ball centered at 0 and of radius xi

    # SUPPORTS only p = 2 and p = Inf for now
    if p == 2:
      r = np.linalg.norm(v,ord=None)
      v = v * min(1, xi/r)
    elif p == np.inf:
        v = np.sign(v) * np.minimum(abs(v), xi)
    else:
         raise ValueError('Values of p different from 2 and Inf are currently not supported...')

    return v

def compute_fooling_rate(net, testloader, v):
    net.eval()
    n_f = 0
    n = len(testloader.dataset)
    with torch.no_grad():
        for img_batch, _ in testloader:
            img_batch = img_batch.cuda()
            per_img_batch = (img_batch + torch.tensor(v).cuda()).cuda()
            est_labels_orig = torch.argmax(net(img_batch), dim=1)
            est_labels_pert = torch.argmax(net(per_img_batch), dim=1)
            fooling = torch.sum(est_labels_pert != est_labels_orig).float() 
            n_f += fooling.item()
    return n_f / n

def universal_perturbation(dataset,
                           val_loader,
                           f,
                           delta=0.17,
                           max_iter_uni = np.inf,
                           xi=10,
                           p=np.inf,
                           num_classes=10,
                           overshoot=0.02,
                           max_iter_df=50):
    """
    :param dataset: Images of size MxHxWxC (M: number of images)
    :param f: feedforward function (input: images, output: values of activation BEFORE softmax).
    :param grads: gradient functions with respect to input (as many gradients as classes).
    :param delta: controls the desired fooling rate (default = 80% fooling rate)
    :param max_iter_uni: optional other termination criterion (maximum number of iteration, default = np.inf)
    :param xi: controls the l_p magnitude of the perturbation (default = 10)
    :param p: norm to be used (FOR NOW, ONLY p = 2, and p = np.inf ARE ACCEPTED!) (default = np.inf)
    :param num_classes: num_classes (limits the number of classes to test against, by default = 10)
    :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).
    :param max_iter_df: maximum number of iterations for deepfool (default = 10)
    :return: the universal perturbation.
    """
    torch.cuda.set_device(1)
    print('p =', p)
    v = 0
    fooling_rate = 0.0
    best_fooling = 0.0
    num_images = len(val_loader.dataset)   # The length of testing data
    round = 0
    foo_rates = []
    att_rates = []

    while fooling_rate < 1-delta:

        print("==========The round", round,"begins=============")
        # Shuffle the dataset
        data_loader = torch.utils.data.DataLoader(dataset, batch_size = 1, shuffle = True, pin_memory=True)

        # Go through the data set and compute the perturbation increments sequentially
        k = 0
        cnt = 0
        f.cuda()
        sum_iter = 0

        for cur_img,_ in tqdm(data_loader):
            k += 1
            cur_img = cur_img.cuda()
            per = Variable( torch.clamp((cur_img + torch.tensor(v).cuda()),0,1), requires_grad = True)

            if int(f(cur_img).argmax()) == int(f(per).argmax()):
                cnt += 1
                # Compute adversarial perturbation
                f.zero_grad()
                dr, iter = deepfool(per,
                                   f,
                                   num_classes = num_classes,
                                   overshoot = overshoot,
                                   max_iter = max_iter_df)
                
                sum_iter += iter
                # Make sure it converged...
                if iter < max_iter_df-1:
                    v = v + dr
                    v = proj_lp(v, xi, p)
        att_rates.append(1-cnt/len(dataset))
        print(" The attack rate on training dataset is:", 1 - cnt/len(dataset))
        print("The average iter is:", sum_iter / cnt)
        print("V is:", abs(v).max())
        # Perturb the dataset with computed perturbation
        # dataset_perturbed = dataset + v

        # Compute the estimated labels in batches
        fooling_rate = compute_fooling_rate(f,val_loader,v) 
        foo_rates.append(fooling_rate)
        print('FOOLING RATE = ', fooling_rate)
        if fooling_rate > best_fooling:
          best_fooling = fooling_rate
          v_opt = v
        print('Best Fooling Rate = ', best_fooling)
           
        round += 1
    return v_opt,foo_rates,att_rates



def main():

    CIFAR10_SUBTRAIN_MEAN = [0.49116027,0.49106753]
    CIFAR10_SUBTRAIN_STD = [0.24728487,0.24676652]
    subset = 1
    set_mean = CIFAR10_SUBTRAIN_MEAN[subset - 1]
    set_std = CIFAR10_SUBTRAIN_STD[subset - 1]
    cifar10_training_loader = get_subtraining_dataloader_cifar10(
            set_mean,
            set_std,
            num_workers=8,
            batch_size=128,
            shuffle=True,
            sub_idx=0
        )
    train_dataset = cifar10_training_loader.dataset
    cifar10_test_loader = get_test_dataloader_cifar10(
            set_mean,
            set_std,
            num_workers=8,
            batch_size=128,
            shuffle=False
        )
    test_dataset = cifar10_test_loader.dataset
    # 加载您的模型，这里假设您已经有一个训练好的模型
    fv = get_model("resnet18",True, 10)
    fv.load_state_dict(torch.load("/home/fazhong/studio/uap/checkpoint/fv_resnet18/Tuesday_19_March_2024_13h_44m_32s/training_session_1-98-best.pth"))


    test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle = False)
    v_opt,foo_rates,att_rates = universal_perturbation(train_dataset,test_loader, fv)
    np.save("v_opt.npy",v_opt)

# 确保当该文件作为脚本运行时才执行 main 函数
if __name__ == "__main__":
    main()